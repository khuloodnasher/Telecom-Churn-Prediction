{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Final-Project-Submission\" data-toc-modified-id=\"Final-Project-Submission-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Final Project Submission</a></span><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Introduction</a></span><ul class=\"toc-item\"><li><span><a href=\"#What-Is-Customer-Churn?\" data-toc-modified-id=\"What-Is-Customer-Churn?-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>What Is Customer Churn?</a></span></li><li><span><a href=\"#Why-it-is-important-to-calculate-customer-churn?\" data-toc-modified-id=\"Why-it-is-important-to-calculate-customer-churn?-1.1.2\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>Why it is important to calculate customer churn?</a></span></li><li><span><a href=\"#Important-Questions:\" data-toc-modified-id=\"Important-Questions:-1.1.3\"><span class=\"toc-item-num\">1.1.3&nbsp;&nbsp;</span>Important Questions:</a></span></li><li><span><a href=\"#About-Telco-data\" data-toc-modified-id=\"About-Telco-data-1.1.4\"><span class=\"toc-item-num\">1.1.4&nbsp;&nbsp;</span>About Telco data</a></span></li></ul></li><li><span><a href=\"#Resources\" data-toc-modified-id=\"Resources-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Resources</a></span></li><li><span><a href=\"#Obtain-Data\" data-toc-modified-id=\"Obtain-Data-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Obtain Data</a></span></li><li><span><a href=\"#Scrub-Data\" data-toc-modified-id=\"Scrub-Data-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Scrub Data</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Check-dublicates\" data-toc-modified-id=\"Check-dublicates-1.4.0.1\"><span class=\"toc-item-num\">1.4.0.1&nbsp;&nbsp;</span>Check dublicates</a></span></li></ul></li></ul></li><li><span><a href=\"#Explore-Data\" data-toc-modified-id=\"Explore-Data-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Explore Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Explore-Target-Column\" data-toc-modified-id=\"Explore-Target-Column-1.5.1\"><span class=\"toc-item-num\">1.5.1&nbsp;&nbsp;</span>Explore Target Column</a></span><ul class=\"toc-item\"><li><span><a href=\"#Exploring-Categorical-Features\" data-toc-modified-id=\"Exploring-Categorical-Features-1.5.1.1\"><span class=\"toc-item-num\">1.5.1.1&nbsp;&nbsp;</span>Exploring Categorical Features</a></span></li></ul></li></ul></li><li><span><a href=\"#Exploring-Numeric-Features\" data-toc-modified-id=\"Exploring-Numeric-Features-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Exploring Numeric Features</a></span><ul class=\"toc-item\"><li><span><a href=\"#Checking-Outliers\" data-toc-modified-id=\"Checking-Outliers-1.6.1\"><span class=\"toc-item-num\">1.6.1&nbsp;&nbsp;</span>Checking Outliers</a></span><ul class=\"toc-item\"><li><span><a href=\"#tenure-outlier-check\" data-toc-modified-id=\"tenure-outlier-check-1.6.1.1\"><span class=\"toc-item-num\">1.6.1.1&nbsp;&nbsp;</span>tenure outlier check</a></span></li><li><span><a href=\"#monthlycharges-outlier-Removal\" data-toc-modified-id=\"monthlycharges-outlier-Removal-1.6.1.2\"><span class=\"toc-item-num\">1.6.1.2&nbsp;&nbsp;</span>monthlycharges outlier Removal</a></span></li><li><span><a href=\"#totalcharges-outlier's-check\" data-toc-modified-id=\"totalcharges-outlier's-check-1.6.1.3\"><span class=\"toc-item-num\">1.6.1.3&nbsp;&nbsp;</span>totalcharges outlier's check</a></span></li></ul></li></ul></li><li><span><a href=\"#Modeling-Data\" data-toc-modified-id=\"Modeling-Data-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>Modeling Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Data-preprocessing\" data-toc-modified-id=\"Data-preprocessing-1.7.1\"><span class=\"toc-item-num\">1.7.1&nbsp;&nbsp;</span>Data preprocessing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Encoding-features\" data-toc-modified-id=\"Encoding-features-1.7.1.1\"><span class=\"toc-item-num\">1.7.1.1&nbsp;&nbsp;</span>Encoding features</a></span></li></ul></li><li><span><a href=\"#Baseline-model\" data-toc-modified-id=\"Baseline-model-1.7.2\"><span class=\"toc-item-num\">1.7.2&nbsp;&nbsp;</span>Baseline model</a></span></li><li><span><a href=\"#Spliting-data-into-Training-and-Testing\" data-toc-modified-id=\"Spliting-data-into-Training-and-Testing-1.7.3\"><span class=\"toc-item-num\">1.7.3&nbsp;&nbsp;</span>Spliting data into Training and Testing</a></span></li><li><span><a href=\"#Select-Best-Model\" data-toc-modified-id=\"Select-Best-Model-1.7.4\"><span class=\"toc-item-num\">1.7.4&nbsp;&nbsp;</span>Select Best Model</a></span></li></ul></li><li><span><a href=\"#Evaluation-Metrics\" data-toc-modified-id=\"Evaluation-Metrics-1.8\"><span class=\"toc-item-num\">1.8&nbsp;&nbsp;</span>Evaluation Metrics</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Build-a--logistic-regression-base-model-using-statsmodels\" data-toc-modified-id=\"Build-a--logistic-regression-base-model-using-statsmodels-1.8.0.1\"><span class=\"toc-item-num\">1.8.0.1&nbsp;&nbsp;</span>Build a  logistic regression base model using statsmodels</a></span></li><li><span><a href=\"#scaling-numeric-features\" data-toc-modified-id=\"scaling-numeric-features-1.8.0.2\"><span class=\"toc-item-num\">1.8.0.2&nbsp;&nbsp;</span>scaling numeric features</a></span></li><li><span><a href=\"#Base-model:logistic-regression-with-sickit-learn-before-smote\" data-toc-modified-id=\"Base-model:logistic-regression-with-sickit-learn-before-smote-1.8.0.3\"><span class=\"toc-item-num\">1.8.0.3&nbsp;&nbsp;</span>Base model:logistic regression with sickit learn before smote</a></span></li><li><span><a href=\"#Interpreting-the-Confusion-Matrix\" data-toc-modified-id=\"Interpreting-the-Confusion-Matrix-1.8.0.4\"><span class=\"toc-item-num\">1.8.0.4&nbsp;&nbsp;</span>Interpreting the Confusion Matrix</a></span></li></ul></li><li><span><a href=\"#False-Negative-Calculation\" data-toc-modified-id=\"False-Negative-Calculation-1.8.1\"><span class=\"toc-item-num\">1.8.1&nbsp;&nbsp;</span>False Negative Calculation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Imbalance-Target\" data-toc-modified-id=\"Imbalance-Target-1.8.1.1\"><span class=\"toc-item-num\">1.8.1.1&nbsp;&nbsp;</span>Imbalance Target</a></span></li><li><span><a href=\"#Using-SMOTE\" data-toc-modified-id=\"Using-SMOTE-1.8.1.2\"><span class=\"toc-item-num\">1.8.1.2&nbsp;&nbsp;</span>Using SMOTE</a></span></li><li><span><a href=\"#Trying-different--models-with-different-parameters\" data-toc-modified-id=\"Trying-different--models-with-different-parameters-1.8.1.3\"><span class=\"toc-item-num\">1.8.1.3&nbsp;&nbsp;</span>Trying different  models with different parameters</a></span></li><li><span><a href=\"#Select-the-Best-Model-with-the-best-parameters\" data-toc-modified-id=\"Select-the-Best-Model-with-the-best-parameters-1.8.1.4\"><span class=\"toc-item-num\">1.8.1.4&nbsp;&nbsp;</span>Select the Best Model with the best parameters</a></span></li></ul></li><li><span><a href=\"#Hypertune_2\" data-toc-modified-id=\"Hypertune_2-1.8.2\"><span class=\"toc-item-num\">1.8.2&nbsp;&nbsp;</span>Hypertune_2</a></span><ul class=\"toc-item\"><li><span><a href=\"#ROC-Curve-for-Random-Forest-Classifier\" data-toc-modified-id=\"ROC-Curve-for-Random-Forest-Classifier-1.8.2.1\"><span class=\"toc-item-num\">1.8.2.1&nbsp;&nbsp;</span>ROC Curve for Random Forest Classifier</a></span></li><li><span><a href=\"#Random-Forest-Tuning-through-pipeline\" data-toc-modified-id=\"Random-Forest-Tuning-through-pipeline-1.8.2.2\"><span class=\"toc-item-num\">1.8.2.2&nbsp;&nbsp;</span>Random Forest Tuning through pipeline</a></span></li></ul></li><li><span><a href=\"#Modeling-with-Ensemble-Methods\" data-toc-modified-id=\"Modeling-with-Ensemble-Methods-1.8.3\"><span class=\"toc-item-num\">1.8.3&nbsp;&nbsp;</span>Modeling with Ensemble Methods</a></span><ul class=\"toc-item\"><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-1.8.3.1\"><span class=\"toc-item-num\">1.8.3.1&nbsp;&nbsp;</span>Conclusion</a></span></li></ul></li><li><span><a href=\"#GradientBoostingClassifier\" data-toc-modified-id=\"GradientBoostingClassifier-1.8.4\"><span class=\"toc-item-num\">1.8.4&nbsp;&nbsp;</span>GradientBoostingClassifier</a></span></li><li><span><a href=\"#AdaBoost\" data-toc-modified-id=\"AdaBoost-1.8.5\"><span class=\"toc-item-num\">1.8.5&nbsp;&nbsp;</span>AdaBoost</a></span><ul class=\"toc-item\"><li><span><a href=\"#Hyperparameter-Tuning\" data-toc-modified-id=\"Hyperparameter-Tuning-1.8.5.1\"><span class=\"toc-item-num\">1.8.5.1&nbsp;&nbsp;</span>Hyperparameter Tuning</a></span></li><li><span><a href=\"#Using-RandomizedSearchCV-to-Save-Time\" data-toc-modified-id=\"Using-RandomizedSearchCV-to-Save-Time-1.8.5.2\"><span class=\"toc-item-num\">1.8.5.2&nbsp;&nbsp;</span>Using RandomizedSearchCV to Save Time</a></span></li></ul></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Project Submission\n",
    "\n",
    "Please fill out: \n",
    "* Student name: Khulood Nasher\n",
    "* Student pace:  online full time\n",
    "* Scheduled project review date/time: \n",
    "* Instructor name: Amber Yandow \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "#### What Is Customer Churn?\n",
    "Customer churn  or  customer attrition is the loss of customers by a business  when customer stop using the service of the company.  We calculate churn rate by dividing the number of customers were lost during that time period by the number of customers were existed at the beginning of that time period.\n",
    "\n",
    "#### Why it is important to calculate customer churn?\n",
    "Because business is built on a number of customers using its service. Keeping its customers is less expensive than bringing new ones.\n",
    "\n",
    "#### Important Questions:\n",
    "Q1: Which customer has a high probability to churn, in other words to leave  telco company and look for another provider?\n",
    "\n",
    "Q2: What are the reasons people are switching or leaving out of the business?\n",
    "\n",
    "Q3 : what can be done to attain custom satisfaction and bring back  old customers and keep the current customers from leaving the business?\n",
    "\n",
    "To answer these questions, Telco retained a data scientist,who his/ her job will predict the customers who have high probability  to churn , and help to set recommendations to keep them.\n",
    "\n",
    "#### About Telco data\n",
    "Each row represents a customer, each column contains customer’s attributes described on the column Metadata.\n",
    "\n",
    "The raw data contains 7043 rows (customers) and 21 columns (features).\n",
    "\n",
    "The “Churn” column is our target.\n",
    "\n",
    "\n",
    "customerID: Customer ID, it is unique value and has  7043 inputs.\n",
    "\n",
    "gender: Whether the customer is a male or a female.\n",
    "\n",
    "\n",
    "SeniorCitizen: Whether the customer is a senior citizen or not (1, 0)\n",
    "\n",
    "\n",
    "Partner:Whether the customer has a partner or not (Yes, No)\n",
    "\n",
    "\n",
    "Dependents: Whether the customer has dependents or not (Yes, No)\n",
    "\n",
    "tenure: Number of months the customer has stayed with the company\n",
    "\n",
    "PhoneService:Whether the customer has a phone service or not (Yes, No)\n",
    "\n",
    "\n",
    "MultipleLines:Whether the customer has multiple lines or not (Yes, No, No phone service)\n",
    "\n",
    "\n",
    "InternetService:Customer’s internet service provider (DSL, Fiber optic, No)\n",
    "\n",
    "\n",
    "OnlineSecurity:Whether the customer has online security or not (Yes, No, No internet service)\n",
    "\n",
    "\n",
    "\n",
    "OnlineBackup:Whether the customer has online backup or not (Yes, No, No internet service)\n",
    "\n",
    "\n",
    "DeviceProtection: Whether the customer has device protection or not (Yes, No, No internet service)\n",
    "\n",
    "\n",
    "\n",
    "TechSupport: Whether the customer has tech support or not (Yes, No, No internet service)\n",
    "\n",
    "\n",
    "\n",
    "StreamingTV: Whether the customer has streaming TV or not (Yes, No, No internet service)\n",
    "\n",
    "\n",
    "\n",
    "StreamingMovies: Whether the customer has streaming movies or not (Yes, No, No internet service)\n",
    "\n",
    "\n",
    "Contract:The contract term of the customer (Month-to-month, One year, Two year)\n",
    "\n",
    "\n",
    "PaperlessBilling:Whether the customer has paperless billing or not (Yes, No)\n",
    "\n",
    "\n",
    "PaymentMethod:The customer’s payment method (Electronic check, Mailed check, Bank transfer (automatic), Credit card (automatic))\n",
    "\n",
    "\n",
    "MonthlyCharges: The amount charged to the customer monthly\n",
    "\n",
    "\n",
    "TotalCharges:The total amount charged to the customer\n",
    "\n",
    "\n",
    "Churn: Whether the customer churned or not (Yes or No)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.298267Z",
     "start_time": "2020-06-01T13:16:39.269834Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'catboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-710f8fcffadb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcatboost\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCatBoostClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStratifiedKFold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'catboost'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels as sm\n",
    "import sklearn.preprocessing as preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import stats\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.328394Z",
     "start_time": "2020-06-01T13:16:39.272Z"
    }
   },
   "outputs": [],
   "source": [
    "## To display  all the interactive output without using the print function\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.329756Z",
     "start_time": "2020-06-01T13:16:39.277Z"
    }
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv('WA_Fn-UseC_-Telco-Customer-Churn.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.331005Z",
     "start_time": "2020-06-01T13:16:39.281Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Dataset shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.332176Z",
     "start_time": "2020-06-01T13:16:39.286Z"
    }
   },
   "outputs": [],
   "source": [
    "list(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.333406Z",
     "start_time": "2020-06-01T13:16:39.290Z"
    }
   },
   "outputs": [],
   "source": [
    "# Checking out columns' (names,types,any missing value)\n",
    "\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### We have 21 columns. Three columns are numeric. One of them is float and the other two are integer. There are 18 object columns. The number of rows represents the number of customers of Telco and they are 7043 customers. There is no missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrub Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ##### Check dublicates\n",
    "check if we have any  duplication  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.334529Z",
     "start_time": "2020-06-01T13:16:39.297Z"
    }
   },
   "outputs": [],
   "source": [
    "df.duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.335708Z",
     "start_time": "2020-06-01T13:16:39.302Z"
    }
   },
   "outputs": [],
   "source": [
    "# checking missing values\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.337089Z",
     "start_time": "2020-06-01T13:16:39.306Z"
    }
   },
   "outputs": [],
   "source": [
    "# lowercase column name\n",
    "df.columns = map(str.lower, df.columns)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.338258Z",
     "start_time": "2020-06-01T13:16:39.311Z"
    }
   },
   "outputs": [],
   "source": [
    "# customerid not useful better to drop it\n",
    "df.drop(['customerid'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.339475Z",
     "start_time": "2020-06-01T13:16:39.315Z"
    }
   },
   "outputs": [],
   "source": [
    "df['totalcharges'].unique()\n",
    "#Replacing spaces with null values in total charges column\n",
    "df['totalcharges'] = df[\"totalcharges\"].replace(\" \",np.nan)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.340722Z",
     "start_time": "2020-06-01T13:16:39.319Z"
    }
   },
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.342160Z",
     "start_time": "2020-06-01T13:16:39.324Z"
    }
   },
   "outputs": [],
   "source": [
    "df['totalcharges'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.343886Z",
     "start_time": "2020-06-01T13:16:39.328Z"
    }
   },
   "outputs": [],
   "source": [
    "print((df['totalcharges'].isna().sum()/len(df))*100) # count the percentage of missing data\n",
    "#Dropping null values from total charges column which contain .15% missing data \n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.345299Z",
     "start_time": "2020-06-01T13:16:39.333Z"
    }
   },
   "outputs": [],
   "source": [
    "# checking missing data\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.346777Z",
     "start_time": "2020-06-01T13:16:39.337Z"
    }
   },
   "outputs": [],
   "source": [
    "#convert to float type\n",
    "df[\"totalcharges\"] = df[\"totalcharges\"].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.347869Z",
     "start_time": "2020-06-01T13:16:39.341Z"
    }
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.349193Z",
     "start_time": "2020-06-01T13:16:39.345Z"
    }
   },
   "outputs": [],
   "source": [
    "df['onlinesecurity'].value_counts()\n",
    "df['onlinebackup'].value_counts()\n",
    "df['deviceprotection'].value_counts()\n",
    "df['techsupport'].value_counts()\n",
    "df['streamingtv'].value_counts()\n",
    "df['streamingmovies'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.350650Z",
     "start_time": "2020-06-01T13:16:39.350Z"
    }
   },
   "outputs": [],
   "source": [
    "#replace 'No internet service' to No for the following columns\n",
    "replace_cols = [ 'onlinesecurity', 'onlinebackup', 'deviceprotection',\n",
    "                'techsupport','streamingtv', 'streamingmovies']\n",
    "for i in replace_cols : \n",
    "    df[i]  = df[i].replace({'No internet service' : 'No'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.352055Z",
     "start_time": "2020-06-01T13:16:39.354Z"
    }
   },
   "outputs": [],
   "source": [
    "df['onlinesecurity'].unique()\n",
    "df['onlinesecurity'].value_counts()\n",
    "df['onlinebackup'].value_counts()\n",
    "df['deviceprotection'].value_counts()\n",
    "df['techsupport'].value_counts()\n",
    "df['streamingtv'].value_counts()\n",
    "df['streamingmovies'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.353606Z",
     "start_time": "2020-06-01T13:16:39.358Z"
    }
   },
   "outputs": [],
   "source": [
    "# replacing values\n",
    "df['seniorcitizen']=df['seniorcitizen'].replace({1:'Yes',0:\"No\"})\n",
    "df['seniorcitizen'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore Target Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.355038Z",
     "start_time": "2020-06-01T13:16:39.364Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df['churn'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.356414Z",
     "start_time": "2020-06-01T13:16:39.368Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(7,7)\n",
    "plt.pie(df[\"churn\"].value_counts(),labels=('not_churn','yes_churn'),explode = [0.1,0],autopct ='%1.1f%%' ,\n",
    "        shadow = True,startangle = 90,labeldistance = 1.1)\n",
    "plt.axis('equal')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  Exploring the target,'churn' column, we can see that we have imbalance target  because of  approximately of 73% of \"No\", and 27% of \"Yes\". This imbalance  will affect our prediction,and must be addressed when modeling through different techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T03:02:02.941652Z",
     "start_time": "2020-05-24T03:02:02.939082Z"
    }
   },
   "source": [
    "##### Exploring Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.357946Z",
     "start_time": "2020-06-01T13:16:39.373Z"
    }
   },
   "outputs": [],
   "source": [
    "### function for plotting caterogical columns\n",
    "def plot_cat(df,feature, xlabel_rotation=0 ):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(14, 4))\n",
    "    \n",
    "    # title for first subplot\n",
    "    ax[0].set(xlabel = f\"{feature}\", ylabel=f\"Number of Customers\")\n",
    "    # title for second subplot\n",
    "    ax[1].set(xlabel = f\"{feature}\", ylabel=f\"Number of Customers\")\n",
    "    \n",
    "    # create subplots, parametr rot=0 - shows how we see labels for axes x\n",
    "    df[df.churn == \"No\"][feature].value_counts().plot(kind='bar', ax=ax[0], rot=xlabel_rotation).set_title('Non churn customers')\n",
    "    df[df.churn == \"Yes\"][feature].value_counts().plot(kind='bar', ax=ax[1], rot=xlabel_rotation).set_title('Churn Customers')\n",
    "    \n",
    "    # main title\n",
    "    # y - how far from plot we put title (if y is small number then title connect with plots, else y is big number then title run away from plot, plot go down the page\n",
    "    # size - font size for title\n",
    "    # weight - type of font\n",
    "    fig.suptitle(f'{feature} distribution in customer attrition', y=1.02, size=16, weight='bold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.359226Z",
     "start_time": "2020-06-01T13:16:39.377Z"
    }
   },
   "outputs": [],
   "source": [
    "# define features\n",
    "categorical_features=['gender', 'seniorcitizen', 'partner', 'dependents',\n",
    "       'phoneservice', 'multiplelines', 'internetservice', 'onlinesecurity',\n",
    "       'onlinebackup', 'deviceprotection', 'techsupport', 'streamingtv',\n",
    "       'streamingmovies', 'contract', 'paperlessbilling', 'paymentmethod']\n",
    "numerical_features=['tenure','monthlycharges', 'totalcharges']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.360791Z",
     "start_time": "2020-06-01T13:16:39.382Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_cat(df,'gender')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " gender: it seems to be an equal distribution of males and females with respect to churn intention.\n",
    " ###### I can guess that gender is not important feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.362040Z",
     "start_time": "2020-06-01T13:16:39.388Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_cat(df,'seniorcitizen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  seniorcitizen: There are much fewer senior citizens and there is a larger proportion of senior citizens churning. in churn plot shows more young people are churning.\n",
    "###### I think seniorcitizen is important feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.363284Z",
     "start_time": "2020-06-01T13:16:39.393Z"
    }
   },
   "outputs": [],
   "source": [
    "# partner \n",
    "plot_cat(df,'partner')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Partner: People with partners and without partners have almost same distribution of not churning. single people have more intention to churn.\n",
    "\n",
    "######  I think partner is also an important feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.364473Z",
     "start_time": "2020-06-01T13:16:39.398Z"
    }
   },
   "outputs": [],
   "source": [
    "# dependents\n",
    "plot_cat(df,'dependents')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Dependents: There are much fewer people with dependents, there is a larger proportion of people with no dependents churning.\n",
    "\n",
    "\n",
    " ###### Dependents looks like an important feature\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.365642Z",
     "start_time": "2020-06-01T13:16:39.402Z"
    }
   },
   "outputs": [],
   "source": [
    "# phoneservice\n",
    "plot_cat(df,'phoneservice')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Phoneservice: There are many more people with a phone service, almost same intention of churn with people having phone service.\n",
    "  \n",
    "###### I can guess that Phoneservice is not important feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.366963Z",
     "start_time": "2020-06-01T13:16:39.407Z"
    }
   },
   "outputs": [],
   "source": [
    "# multiplelines\n",
    "plot_cat(df,'multiplelines')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "multiplelines: The numbers of people who have and do not have multiple lines are almost same with respect to churn intention.\n",
    "\n",
    "######  I can guess multiplelines is not important feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.368137Z",
     "start_time": "2020-06-01T13:16:39.413Z"
    }
   },
   "outputs": [],
   "source": [
    "# internetservice\n",
    "plot_cat(df,'internetservice')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "internetservice: There are many more people who has an internet service either with DSL or fiber but there is a large proportion of people with fiber optic internet service who churn.\n",
    "\n",
    "\n",
    "###### I can expect that internetservice is going to be an important prediction feature especially with Fiber Optic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.369428Z",
     "start_time": "2020-06-01T13:16:39.418Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot_cat(df,'onlinesecurity')\n",
    "\n",
    "plot_cat(df,'onlinesecurity')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OnlineSecurity: there are more people with no online security and a larger proportion of the people has online security, has not churned.\n",
    "\n",
    "customers having online security tend to stay within company compared to customers without online security "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.370694Z",
     "start_time": "2020-06-01T13:16:39.425Z"
    }
   },
   "outputs": [],
   "source": [
    "#onlinebackup\n",
    "plot_cat(df,'onlinebackup')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OnlineBackup:  There are more people with no online backup and  those who has onlinebackup has less probability of churn\n",
    "\n",
    "customers having OnlineBackup tend to stay within company compared to customers without OnlineBackup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.371883Z",
     "start_time": "2020-06-01T13:16:39.430Z"
    }
   },
   "outputs": [],
   "source": [
    "#deviceprotection\n",
    "plot_cat(df,'deviceprotection')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DeviceProtection:  There are more people with no device protection and and those who has DeviceProtection has less probability to churn.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.373176Z",
     "start_time": "2020-06-01T13:16:39.437Z"
    }
   },
   "outputs": [],
   "source": [
    "#techsupport\n",
    "plot_cat(df,'techsupport')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TechSupport: there are more people with no tech support and and those who has tech support has less probability to churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.374278Z",
     "start_time": "2020-06-01T13:16:39.442Z"
    }
   },
   "outputs": [],
   "source": [
    "# streamingtv\n",
    "plot_cat(df,'streamingtv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "StreamingTV: it seems to be almost an equal distribution of people who did and did not have streaming tv with respect to churn intention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.375568Z",
     "start_time": "2020-06-01T13:16:39.448Z"
    }
   },
   "outputs": [],
   "source": [
    "# streamingmovies\n",
    "plot_cat(df,'streamingmovies')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "StreamingMovies: there are more people with no streamingmovies  and those who has streamingmovies has more probability to churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.376924Z",
     "start_time": "2020-06-01T13:16:39.452Z"
    }
   },
   "outputs": [],
   "source": [
    "# contract\n",
    "plot_cat(df,'contract')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contract: There are many more people who are on a month-to-month contract and a large proportion of this group of people has churned.People with one year contract is less churn. People with two years contract are the least people may churn.\n",
    "\n",
    "###### I can tell 'contract'  is one of the most important feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.378236Z",
     "start_time": "2020-06-01T13:16:39.457Z"
    }
   },
   "outputs": [],
   "source": [
    "# paperlessbilling\n",
    "plot_cat(df,'paperlessbilling')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PaperlessBilling: The number of people with paperless billing has quite larger proportion in people who has churned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.379571Z",
     "start_time": "2020-06-01T13:16:39.462Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#paymentmethod\n",
    "plot_cat(df,'paymentmethod',45)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PaymentMethod: There are more people adopting electronic check as a payment method and a large proportion of them has churned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T19:56:41.960452Z",
     "start_time": "2020-05-24T19:56:41.955860Z"
    }
   },
   "source": [
    "In conclusion, based on our analyses,  We can see the more servises added the less people churn.\n",
    "Gender is not a feature, phone service and multipleline arn't service \n",
    "List of important features are :\n",
    "1-seniorcitizen\n",
    "2-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Numeric Features\n",
    "We have three numeric features ['tenure','monthlycharges', 'totalcharges'], we want to study their changes with respect to the target column i.e. 'churn' as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.380886Z",
     "start_time": "2020-06-01T13:16:39.468Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(1, len(numerical_features), figsize=(14, 4))\n",
    "fig.subplots_adjust(wspace=.5)\n",
    "\n",
    "for feature,i in zip(numerical_features,range(len(numerical_features))):\n",
    "    ax[i].set(xlabel = f\"{feature}\", ylabel=f\"Number of Customers of {feature}\")\n",
    "    ax[i].set_title(f'{feature} distribution \\n in customer attrition',size=16, weight='bold')\n",
    "    df[df.churn == \"No\"][feature].hist(bins=30, color=\"blue\", alpha=0.5, ax=ax[i])\n",
    "    df[df.churn == \"Yes\"][feature].hist(bins=30, color=\"red\", alpha=0.5, ax=ax[i])\n",
    "\n",
    "\n",
    "fig.legend([\"Not Churn\",\"Churn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can observe that the greater the totalcharges is, the less churn happened.\n",
    "###### Totalcharges looks very important feature with respect  to churn\n",
    "Totalcharges is skewed to right, taking logarithim will help in normalizing data.\n",
    "* monthly charges is fluctuating with respect to churn.\n",
    "\n",
    "* As customer's tenure increases, the churn mostly decreases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking Outliers\n",
    "To check  for outliers,  I only need to check on my numeric features, and there is no need to check in the categorical features. I will look at it through  a function using scipy library which considers the z-value(number of sigma) = 3 as threshold and returns True if z>3, which means these values are outliers, and return False if z<3, which means these values are not outliers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.382234Z",
     "start_time": "2020-06-01T13:16:39.474Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_outliers(col):\n",
    "    \"\"\"Use scipy to calcualte absolute Z-scores \n",
    "    and return boolean series where True indicates it is an outlier\n",
    "    Args:\n",
    "        col (Series):column from my dataFrame\n",
    "    Returns:\n",
    "        idx_outliers (Series): series of  True/False for each row in col\n",
    "        \n",
    "    Ex:\n",
    "    >> idx_outs = find_outliers(df['tenure'])\n",
    "    >> df_clean = df.loc[idx_outs==False]\"\"\"\n",
    "    from scipy import stats\n",
    "    z = np.abs(stats.zscore(col))\n",
    "    idx_outliers = np.where(z>3,True,False)\n",
    "    return pd.Series(idx_outliers,index=col.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### tenure outlier check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.383716Z",
     "start_time": "2020-06-01T13:16:39.479Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#Checking outliers in tenure\n",
    "\n",
    "idx_outs = find_outliers(df['tenure']) \n",
    "idx_outs.value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T05:51:20.628420Z",
     "start_time": "2020-05-24T05:51:20.624177Z"
    }
   },
   "source": [
    " outliers result is false, this means I don't have outliers in 'tenure'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### monthlycharges outlier Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.385033Z",
     "start_time": "2020-06-01T13:16:39.484Z"
    }
   },
   "outputs": [],
   "source": [
    "#Checking outliers in monthlycharges\n",
    "\n",
    "idx_outs = find_outliers(df['monthlycharges']) \n",
    "idx_outs.value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " outliers result is false, this means I don't have outliers in 'monthlycharges'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### totalcharges outlier's check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.387542Z",
     "start_time": "2020-06-01T13:16:39.491Z"
    }
   },
   "outputs": [],
   "source": [
    "#Checking outliers in totalcharges \n",
    "\n",
    "idx_outs = find_outliers(df['totalcharges']) \n",
    "idx_outs.value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T06:04:45.944793Z",
     "start_time": "2020-05-24T06:04:45.940218Z"
    }
   },
   "source": [
    " outliers result is false, this means I don't have outliers in 'totalcharges'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ##### Encoding features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our target variable (churn) is unbalanced, I will try to build the baseline classification accuracy for X_train through DummyClassifier.\n",
    "\n",
    "I will use the most_frequent strategy of calculating the baseline accuracy.\n",
    "\n",
    "To prepare the dataset for modeling, we need to encode categorical features to numbers. This means encoding \"Yes\", \"No\" to 0 and 1 so that algorithm can work with the data. This process is called onehot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.389713Z",
     "start_time": "2020-06-01T13:16:39.499Z"
    }
   },
   "outputs": [],
   "source": [
    "df['churn'] = df['churn'].apply(lambda x: x.strip().replace(\"Yes\", \"1\").replace(\"No\", \"0\"))\n",
    "df['churn'] = df['churn'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.392691Z",
     "start_time": "2020-06-01T13:16:39.504Z"
    }
   },
   "outputs": [],
   "source": [
    "#### onehot encoding all caterogical data\n",
    "df = pd.get_dummies(df, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.395246Z",
     "start_time": "2020-06-01T13:16:39.509Z"
    }
   },
   "outputs": [],
   "source": [
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To choose the baseline model, I will check first the best model with respect to the accuracy score of cross validation=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spliting data into Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.397272Z",
     "start_time": "2020-06-01T13:16:39.516Z"
    }
   },
   "outputs": [],
   "source": [
    "## Define X, y and split data into training and testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into X and y\n",
    "y=df['churn'].copy()\n",
    "\n",
    "# Define X\n",
    "\n",
    "X = df.drop(columns=['churn'], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "#importing train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Split the data into a training and a test set and set stratify=y to help with imbalance data\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.4,\n",
    "                                                 random_state=42)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T21:40:20.967159Z",
     "start_time": "2020-05-30T21:40:20.964769Z"
    }
   },
   "source": [
    "#### Select Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.399195Z",
     "start_time": "2020-06-01T13:16:39.521Z"
    }
   },
   "outputs": [],
   "source": [
    "#Spot Check Algorithms\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression(solver='liblinear', multi_class='ovr')))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC(gamma='auto')))\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "    # Test options and evaluation metric\n",
    "    kfold = StratifiedKFold(n_splits=10, random_state=1)\n",
    "    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring='accuracy')\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    print('%s: %f (%f)' % (name, cv_results.mean(), cv_results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.401032Z",
     "start_time": "2020-06-01T13:16:39.526Z"
    }
   },
   "outputs": [],
   "source": [
    "#Plotting Best Algorithm\n",
    "from matplotlib import pyplot\n",
    "pyplot.boxplot(results, labels=names)\n",
    "pyplot.title('Algorithm Comparison')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test choose Logistic Regression as the best classifier. I will build a logistic regression model using statsmodels and sickitlearn as my baseline model and git rid of insignificant features based on their p-value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics \n",
    "\n",
    "Important used metrics:\n",
    "    which evaluation metric is most essential for this project ?$$ \\text{Precision} = \\frac{\\text{Number of True Positives}}{\\text{Number of Predicted Positives}} = \\frac{\\text{TP}}{\\text{TP+FP}} $$\n",
    "\n",
    "$$ \\text{Recall} = \\frac{\\text{Number of True Positives}}{\\text{Number of Actual Total Positives}} = \\frac{\\text{TP}}{\\text{TP+FN}}$$\n",
    "$$ \\text{Accuracy} = \\frac{\\text{Number of True Positives + True Negatives}}{\\text{Total Observations}} = \\frac{\\text{TP + TN}}{\\text{TP+TN+FP+FN}}  $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### churned customers are important. We need the model to predict as many churned customers as possible.\n",
    "\n",
    "###### Thus, we cannot afford a high FN and must be low\n",
    "\n",
    "###### Therefore, recall is important here and must be high\n",
    "\n",
    "###### Also, When a model's F1 score is high, we know that the model is doing well all around.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.403271Z",
     "start_time": "2020-06-01T13:16:39.532Z"
    }
   },
   "outputs": [],
   "source": [
    "### define our metrics function with plotting confusion matrix\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix \n",
    "\n",
    "### define function for plotting confusion matrix\n",
    "def plot_confusion_matrix(y_true, y_preds):\n",
    "    # Print confusion matrix\n",
    "    cnf_matrix = confusion_matrix(y_true, y_preds)\n",
    "    # Create the basic matrix\n",
    "    plt.imshow(cnf_matrix,  cmap=plt.cm.Blues)\n",
    "    # Add title and axis labels\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    # Add appropriate axis scales\n",
    "    class_names = set(y) # Get class labels to add to matrix\n",
    "    tick_marks = np.arange(len(class_names))\n",
    "    plt.xticks(tick_marks, class_names, rotation=0)\n",
    "    plt.yticks(tick_marks, class_names)\n",
    "    # Add labels to each cell\n",
    "    thresh = cnf_matrix.max() / 2. # Used for text coloring below\n",
    "    # Here we iterate through the confusion matrix and append labels to our visualization\n",
    "    for i, j in itertools.product(range(cnf_matrix.shape[0]), range(cnf_matrix.shape[1])):\n",
    "            plt.text(j, i, cnf_matrix[i, j],\n",
    "                     horizontalalignment='center',\n",
    "                     color='white' if cnf_matrix[i, j] > thresh else 'black')\n",
    "    # Add a legend\n",
    "    plt.colorbar();\n",
    "    plt.show();\n",
    "def metrics(model_name, y_train, y_test, y_hat_train, y_hat_test):\n",
    "    '''Print out the evaluation metrics for a given models predictions'''\n",
    "    print(f'Model: {model_name}', )\n",
    "    print('-'*60)\n",
    "    plot_confusion_matrix(y_test,y_hat_test)\n",
    "    print(f'test accuracy: {accuracy_score(y_test, y_hat_test)}')\n",
    "    print(f'train accuracy: {accuracy_score(y_train, y_hat_train)}')\n",
    "    print('-'*60)\n",
    "    print('-'*60)\n",
    "    print('Confusion Matrix:\\n', pd.crosstab(y_test, y_hat_test, rownames=['Actual'], colnames=['Predicted'],margins = True))\n",
    "    print('\\ntest report:\\n' + classification_report(y_test, y_hat_test))\n",
    "    print('~'*60)\n",
    "    print('\\ntrain report:\\n' + classification_report(y_train, y_hat_train))\n",
    "    print('-'*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build a  logistic regression base model using statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.404842Z",
     "start_time": "2020-06-01T13:16:39.538Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Build a  logistic regression base model using statsmodels\n",
    "\n",
    "import statsmodels.api as sm\n",
    "# define y\n",
    "y=df['churn'].copy()\n",
    "\n",
    "# Define X\n",
    "\n",
    "X = df.drop(columns=['churn'], axis=1)\n",
    "\n",
    "\n",
    "# Create intercept term required for sm.Logit, see documentation for more information\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit model\n",
    "logit_model = sm.Logit(y, X)\n",
    "\n",
    "# Get results of the fit\n",
    "result = logit_model.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.406564Z",
     "start_time": "2020-06-01T13:16:39.542Z"
    }
   },
   "outputs": [],
   "source": [
    "#dropping  manually columns with p-value >0.05 that were not significant based on their higher p-values.\n",
    "\n",
    "df.drop(['monthlycharges','gender_Male','partner_Yes','dependents_Yes','onlinesecurity_Yes','onlinebackup_Yes',\n",
    " 'deviceprotection_Yes','techsupport_Yes','streamingtv_Yes','streamingmovies_Yes','paymentmethod_Credit card (automatic)',\n",
    " 'paymentmethod_Mailed check'],axis=1, inplace=True)\n",
    "display(df.columns)\n",
    "X=df.drop(columns=['churn'], axis=1) ### redine X after dropping insignificant feature\n",
    "X\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T21:49:28.553321Z",
     "start_time": "2020-05-30T21:49:21.667Z"
    }
   },
   "source": [
    "##### scaling numeric features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.409400Z",
     "start_time": "2020-06-01T13:16:39.547Z"
    }
   },
   "outputs": [],
   "source": [
    "#scale numeric features \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "ss = StandardScaler()\n",
    "# Scale the train and test data\n",
    "X_train = ss.fit_transform(X_train)\n",
    "X_test = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Base model:logistic regression with sickit learn before smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.411389Z",
     "start_time": "2020-06-01T13:16:39.553Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Define X, y and split data into training and testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into X and y\n",
    "y=df['churn'].copy()\n",
    "\n",
    "# Define X\n",
    "\n",
    "X = df.drop(columns=['churn'], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "#importing train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Split the data into a training and a test set and set stratify=y to help with imbalance data\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.4,\n",
    "                                                 random_state=42)\n",
    "\n",
    "df.head()\n",
    "\n",
    "## logistic regression with sickit learn before smote\n",
    "logreg = LogisticRegression()\n",
    "base_log = logreg.fit(X_train, y_train)\n",
    "base_log\n",
    "#predictions\n",
    "y_hat_train=base_log.predict(X_train)\n",
    "y_hat_test = base_log.predict(X_test)\n",
    "\n",
    "# model results\n",
    "\n",
    "metrics(base_log, y_train, y_test, y_hat_train, y_hat_test)\n",
    "# To get the  coffients of all the variables of logistic Regression\n",
    "base_log_cof = pd.Series(base_log.coef_[0], index=X.columns.values)\n",
    "print(base_log_cof)\n",
    "\n",
    "base_log_cof.sort_values(inplace=True)\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.xticks(rotation=90)\n",
    "features=plt.bar(base_log_cof.index,base_log_cof.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Interpreting the Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpreting the the Confusion Matrix results on testing data: 347 out of  734 instances which belong to class 1\"churn\" have been classified as class 0\"not_churn\". In other words,\n",
    "We are classifying 47% of the churn cases as not churn. This is considered a big loss to Telco company. The higher higher recall and precision in non churn customers is not due to correct classification. The model has predicted the majority class for almost all the examples which are the Telco's customers who don't churn. And since about 75.0% of the examples actually belong to this class\"not_churn\", it leads to such high recall and precision  scores. To fix this issue, imbalance data must be addressed through 'smote'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### False Negative Calculation \n",
    "The number of churn customers who have been misclassified as non churn. Having high number of False Negative will cause a big loss to Telco Company.It is considered  a priority job  to a data scientist to come up with a model that predicts very low number of false negative results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.413491Z",
     "start_time": "2020-06-01T13:16:39.559Z"
    }
   },
   "outputs": [],
   "source": [
    "#FN percentage\n",
    "347/734* 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Basemodel \n",
    "Basemodel with logistic regression has an accuracy of 80.5% for training data and 80.3% for testing, The model was able to call more non churn to churn customers . Recall is more important here. The basemodel recall is 53% with false negative pecentage of 347/734 =47%. We have to decrease the FN % and increase Recall for a better model prediction. The  target is imbalanced and can be fixed through smote. I will try Hypertuning the param and and also diffirent algorithm to increase the model's performance with regards testing accuracy and recall. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imbalance Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.415299Z",
     "start_time": "2020-06-01T13:16:39.565Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Churn (Target) Percentage Dustribution \", \"\\n\",round(df['churn'].value_counts(normalize=True) * 100,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The target variable, 'churn', is imbalanced - meaning the variable contains about 3times more “no-churn” instances than “yes-churn” instances. This can present a problem since the positive class we want to predict is the “churn” class (1.0). Because of this imbalanced data, we will make sure that both our training set and testing set maintain this ratio of yes:no churn. This is can be addressed by trying different strategies such as:\n",
    "* class_weight='balanced'\n",
    "* stratify='y'\n",
    "* smote \n",
    "\n",
    "I believe smote is the most powerful, because the other above tow methods is not available with every algorithm.\n",
    "\n",
    "The smote function was imported from the imblearn.over_sampling.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using SMOTE\n",
    "It has been found that balancing the data will do better classification models. We will try balancing our data using SMOTE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.416603Z",
     "start_time": "2020-06-01T13:16:39.571Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "smote\n",
    "refre to install docs \n",
    "https://imbalanced-learn.readthedocs.io/en/stable/install.html\n",
    "'''\n",
    "from imblearn.over_sampling import SMOTE, ADASYN \n",
    "\n",
    "X_resampled, y_resampled = SMOTE().fit_sample(X, y) #adding more users/non-users\n",
    "print(pd.Series(y_resampled).value_counts())\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.418256Z",
     "start_time": "2020-06-01T13:16:39.576Z"
    }
   },
   "outputs": [],
   "source": [
    "#  Logistic Regression after SMOTE\n",
    "logreg_smote1 = LogisticRegression()\n",
    "logreg_smote1.fit(X_train, y_train)\n",
    "\n",
    "#predictions\n",
    "y_hat_train = logreg_smote1.predict(X_train)\n",
    "y_hat_test = logreg_smote1.predict(X_test)\n",
    "\n",
    "# model results\n",
    "metrics(logreg_smote1, y_train, y_test, y_hat_train, y_hat_test)\n",
    "# To get the  coffients of all the variables of logistic Regression\n",
    "logreg_smote1_cof = pd.Series(logreg_smote1.coef_[0], index=X.columns.values)\n",
    "print(logreg_smote1_cof)\n",
    "\n",
    "logreg_smote1_cof.sort_values(inplace=True)\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.xticks(rotation=90)\n",
    "features=plt.bar(logreg_smote1_cof.index,logreg_smote1_cof.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.419812Z",
     "start_time": "2020-06-01T13:16:39.580Z"
    }
   },
   "outputs": [],
   "source": [
    "##Percentage of False Negative\n",
    "249/1305*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Smote improves the balance between  0 and one in Recall, Precision,and F1 in training and testing, and I also notice increase in recalling the one's which is custmoer churn from 53% before smote to 81% after smote and a decrease in number of False Negative percentage from 47% before smote to 19% after smote. However my model accuracy got down from  80.3% to 77.7%. These results are for my first basemodel of logistic regression with the default parameters. To get better model accuracy, and  better recall, I will start my journey of hypertuning with playing with different sets of logistic regression parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.420919Z",
     "start_time": "2020-06-01T13:16:39.585Z"
    }
   },
   "outputs": [],
   "source": [
    "## after smote logistic regression with sickit learn\n",
    "logreg_1 = LogisticRegression(fit_intercept = False, C = 1e15, solver='liblinear')\n",
    "logreg_1.fit(X_train, y_train)\n",
    "\n",
    "#predictions\n",
    "y_hat_train=logreg_1.predict(X_train)\n",
    "y_hat_test = logreg_1.predict(X_test)\n",
    "# model results\n",
    "metrics(logreg_1, y_train, y_test, y_hat_train, y_hat_test)\n",
    "\n",
    "# To get the  coffients of all the variables of logistic Regression\n",
    "model_log_cof = pd.Series(logreg_1.coef_[0], index=X.columns.values)\n",
    "print(model_log_cof)\n",
    "\n",
    "\n",
    "\n",
    "#model_log_cof = pd.Series(model_log_cof.coef_[0], index=X.columns.values)\n",
    "model_log_cof.sort_values(inplace=True)\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.xticks(rotation=90)\n",
    "features=plt.bar(model_log_cof.index,model_log_cof.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T15:17:38.581706Z",
     "start_time": "2020-05-28T15:17:33.423Z"
    }
   },
   "source": [
    "All metrics showed a little  accuracy improvement. Now, I will try hypertuning with different parameters in different models through Gridsearch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Trying different  models with different parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the Gridsearch is going to take too longtime to finish its search and bring up its decision with best model and best paramater, I'm going to spend this time in explaining these models and  parameters  with simple words to make it easier for anyone in the future will read my notebook.\n",
    "\n",
    "SVM Classifier is algorithm used to separate between features through increasing the separating space, which is called the decision boundary between my features. SVM is doing this job using different methods.These methods are called Kernels. Kernels have different forms such as linear, poly, RBF. The linear Kernel is like  a line separating between the features, so the model can separate or classify between feature one and feature 2 using this line. When we do model.predict(X_test), we are telling the model to predict the values of y(churn customer) using (X feature) that has its coffient which is feature weight.As much weight we have, as much the feature will be important. Since the Kernel is linear, the prediction equation(decision function of yes churn or no churn) will be in form of y= intercept+ weight1*X1(feature_name1)+ weight2*X2(feature_name2)+....Weightn*Xn.In other words y= X.W+ b. Those Xs are features that have been standardized  using standard scaler from sickit learn which means they are in same scale of number's range.  If decsion function>0,classification will be positive(yes churn) else will be negative (not churn).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Select the Best Model with the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.422400Z",
     "start_time": "2020-06-01T13:16:39.592Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model_params = {\n",
    "    'svm': {\n",
    "        'model': svm.SVC(gamma='auto'),\n",
    "        'params' : {\n",
    "            'C': [1,10,20],\n",
    "            'kernel': ['rbf','linear']\n",
    "        }  \n",
    "    },\n",
    "    'random_forest': {\n",
    "        'model': RandomForestClassifier(),\n",
    "        'params' : {\n",
    "            'n_estimators': [1,5,10]\n",
    "        }\n",
    "    },\n",
    "    'logistic_regression' : {\n",
    "        'model': LogisticRegression(solver='liblinear',multi_class='auto'),\n",
    "        'params': {\n",
    "            'C': [1,5,10]\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  HyperTune_one with Gridsearch_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.423638Z",
     "start_time": "2020-06-01T13:16:39.597Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "scores = []\n",
    "\n",
    "for model_name, mp in model_params.items():\n",
    "    clf =  GridSearchCV(mp['model'], mp['params'], cv=5, return_train_score=False, n_jobs=-1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    scores.append({\n",
    "        'model': model_name,\n",
    "        'best_score': clf.best_score_,\n",
    "        'best_params': clf.best_params_\n",
    "    })\n",
    "    \n",
    "df = pd.DataFrame(scores,columns=['model','best_score','best_params'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gridsearch shows that Random forest is the best model with cv accuracy of 82.5%. Now I will check the performance of random forest with n=10 and check overfitting, and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.424959Z",
     "start_time": "2020-06-01T13:16:39.603Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate and fit a RandomForestClassifier with n_estimators=100\n",
    "forest_1 = RandomForestClassifier(n_estimators=10)\n",
    "\n",
    "forest_1.fit(X_train, y_train)\n",
    "\n",
    "#predictions\n",
    "y_hat_train=forest_1.predict(X_train)\n",
    "y_hat_test = forest_1.predict(X_test)\n",
    "\n",
    "# model results\n",
    "\n",
    "metrics(forest_1, y_train, y_test, y_hat_train, y_hat_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With applying n-estimator=10 by itself I ran into overfitting problem so, I will do another gridsearch with more parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hypertune_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to follow the Gridsearch best model i.e.Random forest with the best suggested parameter of number of estimators=10,I found my model ran into overfitting issue where the training accuracy is 98.1% while the testing accuracy is 84.6%. I can fix the overfitting of my model through adding more parametetrs to Randomforest which will tune my model and overcome the overfitting issue. I will run a gridsearch again with the following param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.426354Z",
     "start_time": "2020-06-01T13:16:39.609Z"
    }
   },
   "outputs": [],
   "source": [
    "rf_param = RandomForestClassifier()\n",
    "param_grid = {\n",
    "     'criterion':['gini','entropy'],\n",
    "    'max_depth':[2,3,4,5,20],\n",
    "    'min_samples_split':[5,20,50],\n",
    "    'min_samples_leaf':[15,20,30],\n",
    "    'n_estimators': [1,5,10]\n",
    "}\n",
    "gs = GridSearchCV(forest_1, param_grid, cv=3, n_jobs=-1)\n",
    "gs.fit(X_train, y_train)\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gridsearh suggests {'criterion': 'gini','max_depth': 20,'min_samples_leaf': 15 'min_samples_split': 50,'n_estimators': 10} as best parameters. So I will plug them into Randomforest and check the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.427667Z",
     "start_time": "2020-06-01T13:16:39.615Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate and fit a RandomForestClassifier with n_estimators=100\n",
    "forest_2 = RandomForestClassifier(n_estimators=10,\n",
    "                                criterion= 'entropy',\n",
    "                                max_depth= 20,\n",
    "                                min_samples_leaf= 15,\n",
    "                                min_samples_split= 50)\n",
    "\n",
    "forest_2.fit(X_train, y_train)\n",
    "\n",
    "#predictions\n",
    "y_hat_train=forest_2.predict(X_train)\n",
    "y_hat_test = forest_2.predict(X_test)\n",
    "\n",
    "# model results\n",
    "\n",
    "metrics(forest_2, y_train, y_test, y_hat_train, y_hat_test)\n",
    "# To get the feature importance\n",
    "feature_important=forest_2.feature_importances_\n",
    "# Plot features importances\n",
    "imp = pd.Series(data=forest_2.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,12))\n",
    "plt.title(\"Feature importance of Random Forest model\")\n",
    "ax = sns.barplot(y=imp.index, x=imp.values, palette=\"Blues_d\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.428874Z",
     "start_time": "2020-06-01T13:16:39.619Z"
    }
   },
   "outputs": [],
   "source": [
    "# percentage of False negatived\n",
    "177/1305*100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Trying the suggested best parameters above, I got better model testing and training accuracy of 86%. The recall of churnned customer is 86% also. I have  precision, recall, and F1 balance between not churn and yes churn with percentage of  about 86%. The False negative percentage in this model is 14%. I think this is a good model and I will trust it to make my prediction of churn customers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ROC Curve for Random Forest Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.430256Z",
     "start_time": "2020-06-01T13:16:39.625Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "y_score_test = forest.predict(X_test)\n",
    "test_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_score_test)\n",
    "\n",
    "y_score_train = forest.predict(X_train)\n",
    "train_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_score_train)\n",
    "\n",
    "# Train AUC\n",
    "print('Train AUC: {}'.format(auc(train_fpr, train_tpr)))\n",
    "print('Test AUC: {}'.format(auc(test_fpr, test_tpr)))\n",
    "\n",
    "# Seaborn's beautiful styling\n",
    "sns.set_style('darkgrid', {'axes.facecolor': '0.9'})\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "lw = 2\n",
    "\n",
    "plt.plot(train_fpr, train_tpr, color='blue',\n",
    "         lw=lw, label='Train ROC curve')\n",
    "plt.plot(test_fpr, test_tpr, color='darkorange',\n",
    "         lw=lw, label='Test ROC curve')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.yticks([i/20.0 for i in range(21)])\n",
    "plt.xticks([i/20.0 for i in range(21)])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ROC plot shows the true positive rate( the rate that my model predicts the number of customers churnned  to the true churnned  customers) against the false positive rate where my model predicts the number of customers are churnned but the truth they don't churn.The further  the curve is away from the middle line and closer to the top left corner,the better this model precision is.  AUC  is a metric of the model's accuracy and  closer to one is better and closer to 0.5 is bad. Because 0.5 means the model can't differentiate between positive and negative which means the model failed to  make a prediction of whether customer churn or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Random Forest Tuning through pipeline\n",
    "Random Forests is a form of  bagging class that is built on a large collection of trees which is  called number of  esimators, the Random forest decision(yes churn) is built on taking the average of these trees.\n",
    "Now, I will go further steps to do more gridsearch of randomfoest through pipline as an estimator to gridsearch, and random forest is going to be an estimator to pipeline to see if I can  get more metrics improvement,and also to avoid data leakage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.431492Z",
     "start_time": "2020-06-01T13:16:39.631Z"
    }
   },
   "outputs": [],
   "source": [
    "#instantiate the pipeline\n",
    "pipe = Pipeline([('classifier', RandomForestClassifier(random_state=123))])\n",
    "params={'criterion':['gini','entropy'],\n",
    "    'max_depth':[2,3,4,5,7,20],\n",
    "    'min_samples_split':[1,2,5,9,20,50],\n",
    "    'min_samples_leaf':[1,2,3,5,15,20],\n",
    "    'n_estimators':[10, 20, 50, 100, 150, 200]}\n",
    "clf = GridSearchCV(estimator= pipe, param_grid=grid, cv=5, scoring='roc_auc', n_jobs=-1 )\n",
    "clf.fit(X_train, y_train)\n",
    "y_hat_train = clf.predict(X_train)\n",
    "y_hat_test = clf.predict(X_test)\n",
    "print(clf.score(X_train, y_train))\n",
    "print(clf.score(X_test, y_test))\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy score of Gridsearch is pretty high with 93.4% for training and 93.1% for testing. The gridsearch predicts the following parameter as the best parameters for randforest as follows:\n",
    "class_weight= 'balanced',criterion= 'entropy',max_depth= 7,min_samples_leaf= 2,min_samples_split= 10,n_estimators= 200. Now, I will plug them again into randomforest and check the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.432668Z",
     "start_time": "2020-06-01T13:16:39.636Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate and fit a RandomForestClassifier with n_estimators=100\n",
    "forest_3 = RandomForestClassifier(n_estimators=200,\n",
    "                                criterion= 'entropy',\n",
    "                                max_depth= 7,\n",
    "                                min_samples_leaf= 2,\n",
    "                                min_samples_split= 10,\n",
    "                                class_weight= 'balanced')\n",
    "\n",
    "forest_3.fit(X_train, y_train)\n",
    "\n",
    "#predictions\n",
    "y_hat_train=forest_3.predict(X_train)\n",
    "y_hat_test = forest_3.predict(X_test)\n",
    "\n",
    "# model results\n",
    "\n",
    "metrics(forest_3, y_train, y_test, y_hat_train, y_hat_test)\n",
    "# To get the feature importance\n",
    "feature_important=forest_3.feature_importances_\n",
    "# Plot features importances\n",
    "imp = pd.Series(data=forest_3.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,12))\n",
    "plt.title(\"Feature importance of Random Forest model\")\n",
    "ax = sns.barplot(y=imp.index, x=imp.values, palette=\"Blues_d\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.433735Z",
     "start_time": "2020-06-01T13:16:39.640Z"
    }
   },
   "outputs": [],
   "source": [
    "## False Negative percentage\n",
    "158/1305*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.435146Z",
     "start_time": "2020-06-01T13:16:39.645Z"
    }
   },
   "outputs": [],
   "source": [
    "## Plotting ROC Curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "y_score_test = forest_3.predict(X_test)\n",
    "test_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_score_test)\n",
    "\n",
    "y_score_train = forest_3.predict(X_train)\n",
    "train_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_score_train)\n",
    "\n",
    "# Train AUC\n",
    "print('Train AUC: {}'.format(auc(train_fpr, train_tpr)))\n",
    "print('Test AUC: {}'.format(auc(test_fpr, test_tpr)))\n",
    "\n",
    "# Seaborn's beautiful styling\n",
    "sns.set_style('darkgrid', {'axes.facecolor': '0.9'})\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "lw = 2\n",
    "\n",
    "plt.plot(train_fpr, train_tpr, color='blue',\n",
    "         lw=lw, label='Train ROC curve')\n",
    "plt.plot(test_fpr, test_tpr, color='darkorange',\n",
    "         lw=lw, label='Test ROC curve')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.yticks([i/20.0 for i in range(21)])\n",
    "plt.xticks([i/20.0 for i in range(21)])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### SVM classifier \n",
    "Since the Gridsearch suggested SVM as a good classifier with accuracy of 80%, I will try to hypertune it more so I might be able to improve the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling with Ensemble Methods\n",
    "\n",
    " Ensemble Learning:\n",
    " \n",
    " ###### Bagging and Boosting\n",
    "Next, We will try modeling with Ensemble methods, and show the strength of using these powerful methods.\n",
    "I will look for feature importance and improve the accuracy of my ensembled models through tuning hyperparameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T09:12:49.541699Z",
     "start_time": "2020-05-26T09:12:49.537818Z"
    }
   },
   "source": [
    "Ensemble models are combination of simple models working together to make a prediction.\n",
    "\n",
    "• It can be broken down into two tasks:\n",
    "1. Develop a population of base learners\n",
    "2. Combine them to form a composite predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conclusion\n",
    "\n",
    "As we can see, some variables are negatively correlated with the predicted target (Churn), while some others behave positively. Negative correlation means that  churn decreases  when variable increases. \n",
    "\n",
    "tenure is the most important feature for prediction and it has most negative correlation, so when tenure increases, \n",
    "\n",
    "churn decreases, monthly charges is the second important feature, and as monthly charges increase, churn decrease\n",
    "\n",
    "then internetservice_Fiber optic is the third important feature but it has a positive correlation  so as  \n",
    "\n",
    "internetservice_Fiber optic increases,churn increases \n",
    "\n",
    "totalcharges is the forth important feature and it has positive effect. As  totalcharges increase, churn increases                           \n",
    "contract_Two year is the fifth important feature and it has negative effect, so as  contract_Two year increase, churn decreases                           \n",
    "\n",
    "As we have seen in our EDA, having a 2 year contract reduces chances of churn. 2 year contract along with tenure have the most negative relation with Churn as predicted by logistic regressions. Total charges, fibre optic internet services and seniority can lead to higher churn rates. This is interesting because although fibre optic services are faster, customers are likely to churn because of it. I don't understad why this is happening.\n",
    "\n",
    "Till the time let's have a look at other algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.436307Z",
     "start_time": "2020-06-01T13:16:39.653Z"
    }
   },
   "outputs": [],
   "source": [
    "#### Checking important features through SVM classifier with linear kernel\n",
    "from sklearn.svm import SVC #Support vector Machine Classifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "svclinear= SVC(kernel='linear') \n",
    "svclinear.fit(X_train, y_train)\n",
    "#predictions\n",
    "y_hat_train=svclinear.predict(X_train)\n",
    "y_hat_test = svclinear.predict(X_test)\n",
    "\n",
    "\n",
    "# Evaluate predictions\n",
    "print('-'*40)\n",
    "print('Accuracy score for Training Dataset = ', accuracy_score(y_train, y_hat_train))\n",
    "print('Accuracy score for Testing Dataset = ', accuracy_score(y_test, y_hat_test))\n",
    "feature_importance = list(zip(svclinear.coef_[0], X.columns.values))\n",
    "print(feature_importance)\n",
    "# Plot features importances\n",
    "important_features = pd.Series(data=svclinear.coef_[0], index=X.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,12))\n",
    "plt.title(\"Feature importance of Support Vector Machine (linearKernel)\")\n",
    "ax = sns.barplot(y=important_features.index, x=important_features.values, palette=\"Blues_d\", orient='h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.437397Z",
     "start_time": "2020-06-01T13:16:39.657Z"
    }
   },
   "outputs": [],
   "source": [
    "metrics(svclinear,y_train, y_test, y_hat_train, y_hat_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing the SVM  model of  Linear Kernel,RBF Kernel and SVM,  we can see that the accuracy score of testing is around 79%, we don't have overfitting issue here.\n",
    "\n",
    "The best features are: monthly charges, model predicts as monthly charges increase, churn increase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.438674Z",
     "start_time": "2020-06-01T13:16:39.662Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_feature_importances(model):\n",
    "    n_features = X_train.shape[1]\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.barh(range(n_features), model.feature_importances_, align='center') \n",
    "    plt.yticks(np.arange(n_features), X.columns.values) \n",
    "    plt.xlabel('Feature importance')\n",
    "    plt.ylabel('Feature')\n",
    "\n",
    "plot_feature_importances(dtree)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter Tuning through  GridSearchCV and RandomizedSearchCV \n",
    "It is recommended to search the hyper-parameter space for the best cross validation score.it is common that a small subset of those parameters can have a large impact on the predictive or computation performance of the model while others can be left to their default values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GradientBoostingClassifier\n",
    "\n",
    "In scikit-learn we can implement a GradientBoostingClassifier class easily. \n",
    "\n",
    "learning_rate : float, optional (default=0.1)\n",
    "learning rate shrinks the contribution of each tree by`learning_rate`. \n",
    "\n",
    "There is a trade-off between learning_rate and n_estimators.\n",
    "\n",
    "In each stage n_classes_ regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function.\n",
    "\n",
    "Binary classification is a special case where only a single regression tree is induced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.439987Z",
     "start_time": "2020-06-01T13:16:39.667Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gb_clf = GradientBoostingClassifier(\n",
    "max_depth=3,\n",
    "n_estimators=10,\n",
    "learning_rate=0.9\n",
    ")\n",
    "gb_clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_hat_test = gb_clf.predict(X_test)\n",
    "y_hat_train = gb_clf.predict(X_train)\n",
    "\n",
    "# Evaluate predictions\n",
    "print('-'*40)\n",
    "print('Accuracy score for Training Dataset = ', accuracy_score(y_train, y_hat_train))\n",
    "print('Accuracy score for Testing Dataset = ', accuracy_score(y_test, y_hat_test))\n",
    "## overfit issue\n",
    "feature_importances = gb_clf.feature_importances_ \n",
    "features =X.columns\n",
    "plot_feature_importances(gb_clf)\n",
    "important_features = pd.Series(data=gb_clf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "print(important_features)\n",
    "plt.figure(figsize=(10,12))\n",
    "plt.title(\"Feature importance of GradientBoostingClassifier\")\n",
    "ax = sns.barplot(y=imp.index, x=imp.values, palette=\"Blues_d\", orient='h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.441202Z",
     "start_time": "2020-06-01T13:16:39.671Z"
    }
   },
   "outputs": [],
   "source": [
    "#Predict the response for test dataset\n",
    "y_hat_test = gb_clf.predict(X_test)\n",
    "y_hat_train = gb_clf.predict(X_train)\n",
    "\n",
    "# Evaluate predictions\n",
    "print('-'*40)\n",
    "print('Accuracy score for Training Dataset = ', accuracy_score(y_train, y_hat_train))\n",
    "print('Accuracy score for Testing Dataset = ', accuracy_score(y_test, y_hat_test))\n",
    "## overfit issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.442349Z",
     "start_time": "2020-06-01T13:16:39.675Z"
    }
   },
   "outputs": [],
   "source": [
    "model_AdaB = AdaBoostClassifier()\n",
    "model_AdaB.fit(X_train,y_train)\n",
    "y_hat_test  = model_AdaB.predict(X_test)\n",
    "y_hat_train = model_AdaB.predict(X_train)\n",
    "\n",
    "print('-'*40)\n",
    "print('Accuracy score for Training Dataset = ', accuracy_score(y_train, y_hat_train))\n",
    "print('Accuracy score for Testing Dataset = ', accuracy_score(y_test, y_hat_test))\n",
    "importances = model_AdaB.feature_importances_\n",
    "weights_AdaB = pd.Series(importances,\n",
    "                 index=X.columns.values)\n",
    "print(weights_AdaB)\n",
    "plt.figure(figsize=(6, 15))\n",
    "weights_AdaB.sort_values().plot(kind = 'barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AdaBoost\n",
    "Alternatively, rather than attending to the deviance of classification, we may instead weight mis- classified observations at each step. This is how AdaBoost or Adaptive Boosting builds a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.443586Z",
     "start_time": "2020-06-01T13:16:39.681Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ada_boost = AdaBoostClassifier(clf, n_estimators=100, learning_rate=0.3)\n",
    "ada_boost.fit(X_train, y_train)\n",
    "ada_boost.score(X_test, y_test)\n",
    "#predictions\n",
    "y_hat_train=ada_boost.predict(X_train)\n",
    "y_hat_test =ada_boost.predict(X_test)\n",
    "\n",
    "\n",
    "# Evaluate predictions\n",
    "print('-'*40)\n",
    "print('Accuracy score for Training Dataset = ', accuracy_score(y_train, y_hat_train))\n",
    "print('Accuracy score for Testing Dataset = ', accuracy_score(y_test, y_hat_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.444727Z",
     "start_time": "2020-06-01T13:16:39.685Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot feature importance of ada-boost\n",
    "plot_feature_importances(ada_boost)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "I have an overfit with ada_boost  because training score= .88,testing score=.77 with  n_estimators=100, learning_rate=0.3. \n",
    "Now I will investigate to improve the ada boosting accuracy and get rid of overfitting through hypertunning the n_estimators and learning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyperparameter Tuning\n",
    "NOW, I'm going to demonstrate the use of Pipeline, GridSearchCV, and RandomizedSearchCV classes to tune hyperparameters of an AdaBoost model.\n",
    "All available hyperparameters can be tuned, but on my analysis, I will try to explore the learning_rate and n_estimators hyperparameters only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.445789Z",
     "start_time": "2020-06-01T13:16:39.743Z"
    }
   },
   "outputs": [],
   "source": [
    "#import Classes\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.446885Z",
     "start_time": "2020-06-01T13:16:39.747Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set Up Parameter Grid (dict)\n",
    "params = {\n",
    "'ada_boost__learning_rate': np.linspace(0.1, 0.8, 1.0), 'ada_boost__n_estimators': [50, 200, 300, 500, 2000]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.448125Z",
     "start_time": "2020-06-01T13:16:39.750Z"
    }
   },
   "outputs": [],
   "source": [
    "## Instantiate a Pipeline Class with Desired Steps\n",
    "pipe = Pipeline([\n",
    "             ('sscale', StandardScaler()),\n",
    "('ada_boost', AdaBoostClassifier()) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.449399Z",
     "start_time": "2020-06-01T13:16:39.754Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate a GridSearchCV Class, Passing pipe and params\n",
    "grid = GridSearchCV(pipe, param_grid=params)\n",
    "# Building the Model\n",
    "# Fit the GridSearchCV Instance\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "##Access a fit model with best hyperparameters with .best_estimator_ attribute \n",
    "grid.best_estimator_.named_steps['ada_boost']\n",
    "# Score the fit model on testing data\n",
    "grid.score(X_test, y_test)\n",
    "\n",
    "#predictions\n",
    "y_hat_train=grid.predict(X_train)\n",
    "y_hat_test = grid.predict(X_test)\n",
    "\n",
    "\n",
    "# Evaluate predictions\n",
    "print('-'*40)\n",
    "print('Accuracy score for Training Dataset = ', accuracy_score(y_train, y_hat_train))\n",
    "print('Accuracy score for Testing Dataset = ', accuracy_score(y_test, y_hat_test))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " I got better accuracy and less overfitting , and I will try now using samelist of param but with RandomizedSearchCV   and compare the excute time between GridSearchCV and RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using RandomizedSearchCV to Save Time\n",
    "The GridSearchCV class does an exhaustive search of the parameter grid and performs k-fold cross validation.  The time complexity can add up quickly.\n",
    "The RandomizedSearchCV class does not exhaustively search the hyperparameter grid, but of- ten achieves about the same performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.450484Z",
     "start_time": "2020-06-01T13:16:39.791Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate Classes\n",
    "params = {\n",
    "'ada_boost__learning_rate': np.linspace(0.1, 0.8, 1.0),\n",
    "    'ada_boost__n_estimators': [50, 200, 300, 500, 2000]\n",
    "}\n",
    "\n",
    "pipe = Pipeline([\n",
    "             ('sscale', StandardScaler()),\n",
    "('ada_boost', AdaBoostClassifier()) ])\n",
    "rand_grid = RandomizedSearchCV(pipe, param_distributions=params)\n",
    "\n",
    "# Fit the RandomizedSearchCV \n",
    "rand_grid.fit(X_train, y_train)\n",
    "\n",
    "# Instance and Score Model\n",
    "rand_grid.best_estimator_.named_steps['ada_boost']\n",
    "rand_grid.score(X_test, y_test)\n",
    "\n",
    "#predictions\n",
    "y_hat_train=rand_grid.predict(X_train)\n",
    "y_hat_test = rand_grid.predict(X_test)\n",
    "\n",
    "\n",
    "# Evaluate predictions\n",
    "print('-'*40)\n",
    "print('Accuracy score for Training Dataset = ', accuracy_score(y_train, y_hat_train))\n",
    "print('Accuracy score for Testing Dataset = ', accuracy_score(y_test, y_hat_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GridSearchCV executed in 47.5s\n",
    "RandomizedSearchCV executed in 43.6s\n",
    "This diffirence will have bigger effect when im applying the hypertunning in many params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T13:16:40.451990Z",
     "start_time": "2020-06-01T13:16:39.795Z"
    }
   },
   "outputs": [],
   "source": [
    "## modeling with the best estimators learning_rate=0.1, n_estimators=300 and check the accuracy and feature importance\n",
    "ada_boost = AdaBoostClassifier(clf, n_estimators=300, learning_rate=0.1)\n",
    "ada_boost.fit(X_train, y_train)\n",
    "ada_boost.score(X_test, y_test)\n",
    "#predictions\n",
    "y_hat_train=ada_boost.predict(X_train)\n",
    "y_hat_test =ada_boost.predict(X_test)\n",
    "\n",
    "\n",
    "# Evaluate predictions\n",
    "print('-'*40)\n",
    "print('Accuracy score for Training Dataset = ', accuracy_score(y_train, y_hat_train))\n",
    "print('Accuracy score for Testing Dataset = ', accuracy_score(y_test, y_hat_test))\n",
    "### evnthough , I used bestestimators for ADAboost i ran into overfit hers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
